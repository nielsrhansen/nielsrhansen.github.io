<!DOCTYPE html>
<html>
<head>
  <title>Why AIC doesn’t do what you think   and   what can be done about it</title>

  <meta charset="utf-8">
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <meta name="generator" content="pandoc" />




  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">

  <link rel="stylesheet" media="all" href="AIC_files/ioslides-13.5.1/fonts/fonts.css">

  <link rel="stylesheet" media="all" href="AIC_files/ioslides-13.5.1/theme/css/default.css">
  <link rel="stylesheet" media="only screen and (max-device-width: 480px)" href="AIC_files/ioslides-13.5.1/theme/css/phone.css">

  <base target="_blank">

  <script type="text/javascript">
    var SLIDE_CONFIG = {
      // Slide settings
      settings: {
                title: 'Why AIC doesn’t do what you think <br> and <br> what can be done about it',
                        useBuilds: true,
        usePrettify: true,
        enableSlideAreas: true,
        enableTouch: true,
                      },

      // Author information
      presenters: [
            {
        name:  'Niels Richard Hansen' ,
        company: '',
        gplus: '',
        twitter: '',
        www: '',
        github: ''
      },
            ]
    };
  </script>

  <style type="text/css">

    b, strong {
      font-weight: bold;
    }

    em {
      font-style: italic;
    }

    slides > slide {
      -webkit-transition: all 0.4s ease-in-out;
      -moz-transition: all 0.4s ease-in-out;
      -o-transition: all 0.4s ease-in-out;
      transition: all 0.4s ease-in-out;
    }

    .auto-fadein {
      -webkit-transition: opacity 0.6s ease-in;
      -webkit-transition-delay: 0.4s;
      -moz-transition: opacity 0.6s ease-in 0.4s;
      -o-transition: opacity 0.6s ease-in 0.4s;
      transition: opacity 0.6s ease-in 0.4s;
      opacity: 0;
    }

  </style>

  <link rel="stylesheet" href="AIC.css" type="text/css" />

</head>

<body style="opacity: 0">

<slides class="layout-widescreen">

  <slide class="title-slide segue nobackground">
        <!-- The content of this hgroup is replaced programmatically through the slide_config.json. -->
    <hgroup class="auto-fadein">
      <h1 data-config-title><!-- populated from slide_config.json --></h1>
      <h2 data-config-subtitle><!-- populated from slide_config.json --></h2>
      <p data-config-presenter><!-- populated from slide_config.json --></p>
            <p style="margin-top: 6px; margin-left: -2px;">October 25 2016</p>
          </hgroup>
  </slide>

<slide class=''><hgroup><h2>On inference after model selection</h2></hgroup><article  id="on-inference-after-model-selection">

<div>
<em>In a world with a large number of unrelated variables and no clear a priori specifications, uncritical use of standard methods will lead to models that <strong>appear</strong> to have a lot of explanatory power.</em>

<div class="quote">
<p>Freedman, A Note on Screening Regression Equations, 1983</p></div></div>

<div>
<em>There is an abundance of advice on how to perform the mechanics of choosing a model, &#8230;. There is a dearth of respectable theory, or even trustworthy advice, &#8230;</em>

<div class="quote">
<p>Miller, Subset Selection in Regression, 1990</p></div></div>

<div>
<em>This usage [ad hoc inference after model selection] has long been a quiet scandal in the statistical community.</em>

<div class="quote">
<p>Breiman, The Little Bootstrap and Other Methods for Dimensionality Selection in Regression: X-Fixed Prediction Error, 1992</p></div></div>

</article></slide><slide class=''><hgroup><h2>On inference after model selection</h2></hgroup><article  id="on-inference-after-model-selection-1">

<div>
<em>What has happened in this field since the first edition was published in 1990? The short answer is that there has been very little real progress.</em>

<div class="quote">
<p>Miller, Subset Selection in Regression, Second Edition, 2000</p></div></div>

<div>
<em>It [stepwise variable selection] &#8230; violates every principle of statistical estimation and hypothesis testing.</em>

<div class="quote">
<p>Harrell, Regression Modeling Strategies, 2001</p></div></div>

<div>
<em>Do not include test statistics and P-values when using the information-theoretic approach as this inappropriately mixes differing analysis paradigms.</em>

<div class="quote">
<p>Burnham and Anderson, Model Selection and Multimodel Inference, 2002</p></div></div>

<div>
<em>Currently, there is no overarching theory for inference after model selection.</em>

<div class="quote">
<p>Efron and Hastie, Computer Age Statistical Inference, 2016</p></div></div>

</article></slide><slide class=''><hgroup><h2>First take home message</h2></hgroup><article  id="first-take-home-message">

<div>
<p>The following two-step procedure</p>

<ul>
<li>perform data driven model selection (e.g. minimize AIC)</li>
<li>compute and report statistics <strong>as if the model were not selected</strong></li>
</ul>

<p>is wrong and can be grossly misleading.</p></div>

<p>Biased estimates, over-optimistic assessment of model fit and predictive ability, and under-estimation of uncertainty are some of the consequences.</p>

<p><strong>Don&#39;t use, promote or teach this two-step procedure!</strong></p>

</article></slide><slide class=''><hgroup><h2>AIC for Gaussian regression (fixed variance)</h2></hgroup><article  id="aic-for-gaussian-regression-fixed-variance">

<p>If \(Y \sim \mathcal{N}(\xi, \sigma^2 I)\) then \[\textrm{AIC} = ||Y - \hat{\xi}||^2 / \sigma^2 + 2 d\] when \(\sigma^2\) is fixed and \(\hat{\xi}\) is the least squares estimator in a subset of dimension \(d\).</p>

<p>Fix \(\sigma^2 = 1\) from hereon, and for \(\lambda \in \Lambda\) (and index set) let \[\textrm{AIC}(\lambda) = ||Y - \hat{\xi}(\lambda)||^2 + 2 d(\lambda).\]</p>

<p>Consider \(\xi = X \beta\) for \(X\) and \(n \times p\) matrix and \(\beta \in \mathbb{R}^p\).</p>

</article></slide><slide class=''><hgroup><h2>n = 25, p = 5</h2></hgroup><article  id="n-25-p-5">

<p><img src="AIC_files/figure-html/p1-1.png" width="900" style="display: block; margin: auto;" /></p>

</article></slide><slide class=''><hgroup><h2>n = 25, p = 20</h2></hgroup><article  id="n-25-p-20">

<p><img src="AIC_files/figure-html/p2-1.png" width="900" style="display: block; margin: auto;" /></p>

</article></slide><slide class=''><hgroup><h2>n = 25, p = 5</h2></hgroup><article  id="n-25-p-5-1">

<p><img src="AIC_files/figure-html/p3-1.png" width="900" style="display: block; margin: auto;" /></p>

</article></slide><slide class=''><hgroup><h2>n = 25, p = 20</h2></hgroup><article  id="n-25-p-20-1">

<p><img src="AIC_files/figure-html/p4-1.png" width="900" style="display: block; margin: auto;" /></p>

</article></slide><slide class=''><hgroup><h2>Model weights and model averaging</h2></hgroup><article  id="model-weights-and-model-averaging">

<p>Introduce weights \[w_{\gamma}(\lambda) = \frac{\exp(- \gamma \textrm{AIC}(\lambda))}{\int \exp(- \gamma \textrm{AIC}(\lambda)) \pi(\mathrm{d}\lambda)}.\]</p>

<ul>
<li>\(\gamma = 1/2\) has a Bayes interpretation</li>
<li>\(\gamma \to 0\) gives all models the same weight</li>
<li>\(\gamma \to \infty\) concentrates the weights on the models with minimal AIC.</li>
</ul>

<p>\[\hat{\xi}_{\gamma} = \int \hat{\xi}(\lambda) w_{\gamma}(\lambda) \pi(\mathrm{d} \lambda)\] is the model averaging estimator.</p>

</article></slide><slide class=''><hgroup><h2>n = 25, p = 5, averaging</h2></hgroup><article  id="n-25-p-5-averaging">

<p><img src="AIC_files/figure-html/p5-1.png" width="900" style="display: block; margin: auto;" /></p>

</article></slide><slide class=''><hgroup><h2>n = 25, p = 20, averaging</h2></hgroup><article  id="n-25-p-20-averaging">

<p><img src="AIC_files/figure-html/p6-1.png" width="900" style="display: block; margin: auto;" /></p>

</article></slide><slide class=''><hgroup><h2>n = 25, p = 5, averaging</h2></hgroup><article  id="n-25-p-5-averaging-1">

<p><img src="AIC_files/figure-html/p7-1.png" width="900" style="display: block; margin: auto;" /></p>

</article></slide><slide class=''><hgroup><h2>n = 25, p = 20, averaging</h2></hgroup><article  id="n-25-p-20-averaging-1">

<p><img src="AIC_files/figure-html/p8-1.png" width="900" style="display: block; margin: auto;" /></p>

</article></slide><slide class=''><hgroup><h2>Summary</h2></hgroup><article  id="summary">

<div>
<p>Model selection estimators</p>

<ul>
<li>have highly non-Gaussian distributions, which are difficult to use for confidence intervals.</li>
<li>are biased &#8211; conditionally on selection &#8211; unless the true effect is large.</li>
</ul></div>

<div>
<p>Model averaging estimators</p>

<ul>
<li>reduce variance via shrinkage for small effects.</li>
<li>have a distribution &#8211; though non-Gaussian &#8211; that can be used for confidence intervals (at least via bootstrapping).</li>
</ul></div>

</article></slide><slide class=''><hgroup><h2>Second take home message</h2></hgroup><article  id="second-take-home-message">

<p>But isn&#39;t model selection based on AIC sensible?</p>

<div>
<em>Regardless of asymptotic optimality results, criterion or estimates such as \(C_p\), AIC, BIC, and so on are highly biased in finite data situations because they do not account for the data-driven selection.</em>

<div class="quote">
<p>Breiman, The Little Bootstrap and Other Methods for Dimensionality Selection in Regression: X-Fixed Prediction Error, 1992</p></div></div>

<p>AIC is a prediction error estimate for a <strong>fixed model</strong>.</p>

<p><strong>AIC doesn&#39;t adjust for model selection.</strong></p>

</article></slide><slide class=''><hgroup><h2>AIC as a prediction error estimate</h2></hgroup><article  id="aic-as-a-prediction-error-estimate">

<div>
<p>Let \(Y \perp \!\! \perp Y^{\textrm{New}}\) and \(Y \overset{\mathcal{D}}{=} Y^{\textrm{New}}\).</p>

<p>If \(Y \sim (\xi, I)\), \(\hat{\xi}(\lambda) = S_{\lambda} Y\) and \(d(\lambda) = \mathrm{tr}(S_{\lambda})\) then \[E (\textrm{AIC}(\lambda)) = E || Y^{\mathrm{New}} -  \hat{\xi}(\lambda)||^2 
= n + \underbrace{E || \xi -  \hat{\xi}(\lambda)||^2}_{\textrm{MSE}}.\]</p></div>

<p>Thus \[\textrm{AIC}(\lambda) - n = ||Y - S_{\lambda}Y||^2 + 2 d(\lambda) - n\] is an unbiased estimate of MSE.</p>

</article></slide><slide class=''><hgroup><h2>Forward stepwise variable selection</h2></hgroup><article  id="forward-stepwise-variable-selection">

<p>If \(S_{\lambda}\) is a <strong>fixed</strong> projection then \(d(\lambda) = \textrm{rank}(S_{\lambda})\).</p>

<p>Forward stepwise variable selection results in a sequence of projections \[S_{0}, \ldots, S_{p}\] onto nested subspaces.</p>

<p>The projection \(S_d\) includes the predictor not included by \(S_{d-1}\) that minimizes the residual squared error.</p>

<p>It follows that \(S_d\) is <strong>selected in a data dependent way</strong>.</p>

</article></slide><slide class=''><hgroup><h2>n = 100, p = 50</h2></hgroup><article  id="n-100-p-50">

<p><img src="AIC_files/figure-html/unnamed-chunk-1-1.png" width="900" style="display: block; margin: auto;" /></p>

</article></slide><slide class=''><hgroup><h2>n = 100, p = 50</h2></hgroup><article  id="n-100-p-50-1">

<p><img src="AIC_files/figure-html/unnamed-chunk-2-1.png" width="900" style="display: block; margin: auto;" /></p>

</article></slide><slide class=''><hgroup><h2>n = 100, p = 50</h2></hgroup><article  id="n-100-p-50-2">

<p><img src="AIC_files/figure-html/unnamed-chunk-3-1.png" width="900" style="display: block; margin: auto;" /></p>

<!-- ## Nonlinear estimators -->

<!-- It even holds that  -->

<!-- $$\textrm{MSE} = E (\textrm{AIC}(\lambda)) - n = E(||Y - f_{\lambda}(Y)||^2 + 2 d(\lambda)) - n .$$ -->

<!-- for nonlinear estimators $\hat{\xi}(\lambda) = f_{\lambda}(Y)$ with -->

<!-- $$d(\lambda) = \mathrm{div}(f_{\lambda})(Y) = \sum_{j=1}^n \partial_j f_{\lambda}(Y) = \mathrm{tr}(D f_{\lambda}(Y)).$$ -->

<!-- provided $Y \sim \mathcal{N}(\xi, I)$ and $f_{\lambda}$ is **differentiable.** -->

</article></slide><slide class=''><hgroup><h2>Generalized information criteria</h2></hgroup><article  id="generalized-information-criteria">

<p>\[\textrm{IC} = ||Y - \hat{\xi}||^2 + c(d, n, p).\] <strong>Akaike:</strong> \(c(d, n, p) = 2 d.\)</p>

<p><strong>Bayes:</strong> \(c(d, n, p) = \log(n) d.\)</p>

<p><strong>Hannan–Quinn:</strong> \(c(d, n, p) = 2 \log(\log(n)) d\)</p>

<p><strong>Risk inflation:</strong> \(c(d, n, p) = (2 \log(p)) d\)</p>

<p>We propose a new choice of \(c(d, n, p)\).</p>

</article></slide><slide class=''><hgroup><h2>Model selection</h2></hgroup><article  id="model-selection">

<p><img src="modelSelectv2.png" width="600" style="display: block; margin: auto;" /> Model selection results in <strong>nonlinear</strong> and <strong>discontinuous</strong> estimators.</p>

<p>\[E || Y^{\mathrm{New}} -  \hat{\xi}(\lambda)||^2 = E(\mathrm{AIC}(\lambda)) + 2 q \lambda \partial_{\lambda} E(d(\lambda)) + R\] for some model selection estimators. <em>Debiased lasso</em>: \(R = 0\) and \(q = 1\).</p>

</article></slide><slide class=''><hgroup><h2>Forward stepwise variable selection</h2></hgroup><article  id="forward-stepwise-variable-selection-1">

<p>Define for \(\lambda \geq 0\) \[
\DeclareMathOperator*{\argmin}{arg\,min}
d(\lambda) = \argmin_{d = 0, \ldots, p} ||Y - S_d Y||^2 + \lambda d.
\]</p>

<p>Then \(S_{d(\lambda)}\) has prediction error as above with \(q = 2\) and \(R \to 0\) for \(X \to \textrm{orthogonal design}.\)</p>

<p>It gives generalized information criterion and model average \[
\begin{aligned}
\textrm{IC}(\lambda) &amp; = ||Y - S_{d(\lambda)} Y||^2 + 2(d(\lambda) + 2 \hat{\partial}) \\
\tilde{\xi}_{\gamma} &amp; = \frac{\int \hat{\xi}(\lambda) \exp(- \gamma \textrm{IC}(\lambda))\pi(\mathrm{d} \lambda)}{\int \exp(- \gamma \textrm{IC}(\lambda)) \pi(\mathrm{d}\lambda)}
\end{aligned}
\]</p>

</article></slide><slide class=''><hgroup><h2>n = 100, p = 50, \(\gamma = 0.1\)</h2></hgroup><article  id="n-100-p-50-gamma-0.1">

<p><img src="AIC_files/figure-html/unnamed-chunk-5-1.png" width="900" style="display: block; margin: auto;" /></p>

</article></slide><slide class=''><hgroup><h2>n = 100, p = 50, \(\gamma = 0.1\)</h2></hgroup><article  id="n-100-p-50-gamma-0.1-1">

<p><img src="AIC_files/figure-html/unnamed-chunk-6-1.png" width="900" style="display: block; margin: auto;" /></p>

</article></slide><slide class=''><hgroup><h2>n = 100, p = 50, \(\gamma = 0.1\)</h2></hgroup><article  id="n-100-p-50-gamma-0.1-2">

<p><img src="AIC_files/figure-html/unnamed-chunk-7-1.png" width="900" style="display: block; margin: auto;" /></p>

</article></slide><slide class=''><hgroup><h2>Conclusion</h2></hgroup><article  id="conclusion">

<ul>
<li>Model averaging is better than model selection.</li>
<li>AIC <strong>isn&#39;t</strong> suitable when data adaptive model selection is performed.</li>
<li>A new information criterion gives better model selection and model averaging performance.</li>
<li>Practical alternatives are resampling and refitting schemes (computationally heavier).</li>
</ul>

<p>Frederik Riis Mikkelsen and NRH. <a href='https://arxiv.org/abs/1601.03524' title=''>Degrees of Freedom for Piecewise Lipschitz Estimators</a>. arXiv:1601.03524</p>

<p>NRH and Alexander Sokol. <a href='https://arxiv.org/abs/1402.2997' title=''>Degrees of freedom for nonlinear least squares estimation</a>. arXiv:1402.2997</p>

</article></slide><slide class=''><hgroup><h2>Thanks</h2></hgroup><article  id="thanks">

<em>Standard statistical practice ignores model uncertainty. Data analysts typically select a model from some class of models and then proceed as if the selected model had generated the data. This approach ignores the uncertainty in model selection, leading to over-confident inferences and decisions that are more risky than one thinks they are.</em>

<div class="quote">
<p>Hoeting <em>et al.</em>, Bayesian Model Averaging: A Tutorial, 1999</p></div>

<div style="text-align: center; padding-top: 30px;">
<p><strong>Let&#39;s do better!</strong></p></div>

<div style="padding-top: 30px">
<p>Source code: <a href='https://github.com/nielsrhansen/InformationCriteria' title=''>https://github.com/nielsrhansen/InformationCriteria</a></p></div></article></slide>


  <slide class="backdrop"></slide>

</slides>

<script src="AIC_files/ioslides-13.5.1/js/modernizr.custom.45394.js"></script>
<script src="AIC_files/ioslides-13.5.1/js/prettify/prettify.js"></script>
<script src="AIC_files/ioslides-13.5.1/js/prettify/lang-r.js"></script>
<script src="AIC_files/ioslides-13.5.1/js/prettify/lang-yaml.js"></script>
<script src="AIC_files/ioslides-13.5.1/js/hammer.js"></script>
<script src="AIC_files/ioslides-13.5.1/js/slide-controller.js"></script>
<script src="AIC_files/ioslides-13.5.1/js/slide-deck.js"></script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "AIC_files/mathjax-local/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

<!-- map slide visiblity events into shiny -->
<script>
  (function() {
    if (window.jQuery) {
       window.jQuery(document).on('slideleave', function(e) {
         window.jQuery(e.target).trigger('hidden');
      });
       window.jQuery(document).on('slideenter', function(e) {
         window.jQuery(e.target).trigger('shown');
      });
    }
  })();
</script>

</body>
</html>
