<!DOCTYPE html>
<html>
  <head>
    <title>Bivariate smoothing</title>
    <meta charset="utf-8">
    <meta name="author" content="Niels Richard Hansen" />
    <meta name="date" content="2017-09-13" />
    <link href="BivariateSmoothers_files/remark-css-0.0.1/example.css" rel="stylesheet" />
    <link rel="stylesheet" href="Science.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Bivariate smoothing
### Niels Richard Hansen
### September 13, 2017

---


## A scatter plot




```r
p &lt;- qplot(x, y, data = bivar); p
```

&lt;img src="BivariateSmoothers_files/figure-html/unnamed-chunk-2-1.png" width="600" height="400" style="display: block; margin: auto;" /&gt;

---
## A linear fit


```r
p + geom_smooth(method = "lm")
```

&lt;img src="BivariateSmoothers_files/figure-html/unnamed-chunk-3-1.png" width="600" height="400" style="display: block; margin: auto;" /&gt;

---
## A polynomial fit


```r
p + geom_smooth(method = "lm", formula = y ~ poly(x, 5))
```

&lt;img src="BivariateSmoothers_files/figure-html/unnamed-chunk-4-1.png" width="600" height="400" style="display: block; margin: auto;" /&gt;

---
## Another polynomial fit


```r
p + geom_smooth(method = "lm", formula = y ~ poly(x, 20))
```

&lt;img src="BivariateSmoothers_files/figure-html/unnamed-chunk-5-1.png" width="600" height="400" style="display: block; margin: auto;" /&gt;

---
## A spline smooth


```r
p + geom_smooth(method = "gam", formula = y ~ s(x))
```

&lt;img src="BivariateSmoothers_files/figure-html/unnamed-chunk-6-1.png" width="600" height="400" style="display: block; margin: auto;" /&gt;

---
## A loess smooth


```r
p + geom_smooth(method = "loess")  ## Actually the default
```

&lt;img src="BivariateSmoothers_files/figure-html/unnamed-chunk-7-1.png" width="600" height="400" style="display: block; margin: auto;" /&gt;


---
## Smoothing with ggplot2 

The `geom_smooth` function easily adds misc. model fits or scatter plot smoothers
to the scatter plot.
--


The `stat_smooth` function is reponsible for delegating the computations. 
See `?stat_smooth`.
--


Spline smoothing is performed via the `gam` function in the mgcv package, whereas
loess smoothing is via the `loess` function in the stats package. 
--


Any "smoother" can be used that supports a formula interface and has a prediction 
function adhering to the standards of `predict.lm`.

---
## Running mean 

Implementation of formula (11.6) assuming `\(y\)` in correct order.


```r
runMean &lt;- function(y, k) {
  n &lt;- length(y)
  m &lt;- floor((k - 1) / 2)
  k &lt;- 2 * m + 1
  y &lt;- y / k
  s &lt;- rep(NA, n)
  s[m + 1] &lt;- sum(y[1:k])
  for(i in (m + 1):(n - m - 1)) 
    s[i + 1] &lt;- s[i] - y[i - m] + y[i + 1 + m]
  s
}
```

---
## An interface for `geom_smooth`.


```r
rMean &lt;- function(..., data, k = 5) {
  ord &lt;- order(data$x)
  structure(list(x = data$x[ord], y = runMean(data$y[ord], k = k)), 
            class = "rMean")
}
predict.rMean &lt;- function(object, newdata, ...) 
  approx(object$x, object$y, newdata$x)$y ## Linear interpolation
```

---
## A running mean


```r
p + stat_smooth(method = "rMean", se = FALSE, n = 200)
```

&lt;img src="BivariateSmoothers_files/figure-html/unnamed-chunk-10-1.png" width="600" height="400" style="display: block; margin: auto;" /&gt;

---
## Figure 11.2


```r
p + stat_smooth(method = "rMean", se = FALSE, n = 200, 
                method.args = list(k = 13))
```

&lt;img src="BivariateSmoothers_files/figure-html/unnamed-chunk-11-1.png" width="600" height="400" style="display: block; margin: auto;" /&gt;

---
## Boundary


```r
rMean &lt;- function(..., data, k = 5, boundary = NULL) {
  ord &lt;- order(data$x)
  y &lt;- data$y[ord]
  n &lt;- length(y)
  m &lt;- floor((k - 1) / 2)
  if (m &gt; 0 &amp; !is.null(boundary)) {
    if (boundary == "pad")
      y &lt;- c(rep(y[1], m), y, rep(y[n], m))
    if (boundary == "rev")
      y &lt;- c(y[m:1], y, y[n:(n - m + 1)])
  }    
  s &lt;- runMean(y, k = k)
  if(!is.null(boundary))
    s &lt;- na.omit(s)
  structure(list(x = data$x[ord], y = s), class = "rMean")
}
```

---
## No boundary

&lt;img src="BivariateSmoothers_files/figure-html/unnamed-chunk-13-1.png" width="600" height="400" style="display: block; margin: auto;" /&gt;

---
## Boundary, padding

&lt;img src="BivariateSmoothers_files/figure-html/unnamed-chunk-14-1.png" width="600" height="400" style="display: block; margin: auto;" /&gt;

---
## Boundary, reversion

&lt;img src="BivariateSmoothers_files/figure-html/unnamed-chunk-15-1.png" width="600" height="400" style="display: block; margin: auto;" /&gt;

---
## Mean squared prediction error


`\begin{aligned}
E((Y-\hat{s}_k(x_0))^2 &amp; \mid X=x_0) \\ &amp; =  E((Y-s(x_0))^2 \mid X=x_0) \\ 
&amp; \ \ + E((s(x_0)-E(\hat{s}_k(x_0)))^2 \mid X=x_0) \\ 
&amp; \ \ + E((\hat{s}_k(x_0)-E(\hat{s}_k(x_0)))^2 \mid X=x_0) \\
&amp; = \sigma^2 + \underbrace{\left[s(x_0) - \frac{1}{k} \sum_{l \in
    N_k(x_0)} s(x_l)\right]^2}_{\textrm{squared bias}} + 
    \underbrace{\frac{\sigma^2}{k}}_{\textrm{variance}}
\end{aligned}`

Here `\(s(x_0) = E(Y \mid X = x_0)\)`.

---
## LOOCV 

The running mean / nearest neighbour smoother is a *linear smoother*,
`\(\hat{\mathbf{s}} = \mathbf{S} \mathbf{Y}.\)`
--


How to predict `\(Y_i\)` if it is left out?
--


A *definition* for a linear smoother is 
`$$\hat{s}^{(-i)}_i = \sum_{j \neq i} \frac{S_{ij}Y_j}{1 - S_{ii}}.$$`
--


For many smoothing procedures with a natural "out-of-sample" prediction method
the identity above holds. 
--

It follows that for *leave-one-out cross validation*
`$$\mathrm{CVRSS} = \sum_{i} (Y_i - \hat{s}^{(-i)}_i)^2 = 
\sum_{i} \left(\frac{Y_i - \hat{s}_i}{1 - S_{ii}}\right)^2$$`

---
##LOOCV 


```r
cvrss &lt;- function(k, y) {
  s &lt;- sapply(k, function(k) runMean(y, k))
  rowMeans(t((y - s)^2) / (1 - 1/k)^2, na.rm = TRUE) 
}
```
--


```r
cvrss2 &lt;- function(k, y) {
  s &lt;- sapply(k, function(k) 
    rMean(data = bivar, k = k, boundary = "pad")$y)
  rowMeans(t((y - s)^2) / (1 - 1/k)^2, na.rm = TRUE)
}
```


---
## LOOCV - NA boundary


```r
kk &lt;- seq(3, 51, 2)
qplot(kk, cvrss(kk, bivar$y)) + geom_line() + ylim(1.8, 3.2)
```

&lt;img src="BivariateSmoothers_files/figure-html/unnamed-chunk-18-1.png" width="600" height="400" style="display: block; margin: auto;" /&gt;

---
## LOOCV - boundary 


```r
kk &lt;- seq(3, 51, 2)
qplot(kk, cvrss2(kk, bivar$y)) + geom_line() + ylim(1.8, 3.2)
```

&lt;img src="BivariateSmoothers_files/figure-html/unnamed-chunk-19-1.png" width="600" height="400" style="display: block; margin: auto;" /&gt;

---
## A general decomposition 

For a linear smoother `\(\hat{\mathbf{s}} = \mathbf{S} \mathbf{Y}\)` and with `\(\mathrm{var}(Y_i) = \sigma^2\)`,
`\(\mathbf{Y}^{\text{new}} \overset{\mathcal{D}}{=} \mathbf{Y}\)` and 
`\(\mathbf{Y}^{\text{new}} \perp \! \! \perp \mathbf{Y}\)` 
--


$$
E(||\mathbf{Y}^{\text{new}}-\hat{\mathbf{s}}||^2 ) =
 n \sigma^2 + \underbrace{||(I-\mathbf{S})\mathbf{s}||^2}_{\text{bias}^2} + \sigma^2
  \text{trace}(\mathbf{S}^T\mathbf{S}).
$$
conditionally on `\(\mathbf{S}\)`.
--

While

$$
E(||\mathbf{Y} - \hat{\mathbf{s}}||^2 ) =
 n \sigma^2 + \text{bias}^2 + \sigma^2(
  \text{trace}(\mathbf{S}^T\mathbf{S}) - 2 \text{trace}(\mathbf{S})).
$$

---
## MSPE 

From the formulas above 
`$$E(||\mathbf{Y}^{\text{new}}-\hat{\mathbf{s}}||^2 ) = E(||\mathbf{Y} - \hat{\mathbf{s}}||^2 ) + 2 \sigma^2 \text{trace}(\mathbf{S}),$$`
--


which gives the following estimate of mean squared prediction error
`$$\widehat{\mathrm{MSPE}} = ||\mathbf{Y} - \hat{\mathbf{s}}||^2  + 2 \hat{\sigma}^2  \text{trace}(\mathbf{S}).$$`

---
## Variance estimation 
For a *low-bias* smoother, 
`$$E(||\mathbf{Y} - \hat{\mathbf{s}}||^2 ) \simeq
 \sigma^2 (n + \text{trace}(\mathbf{S}^T\mathbf{S}) - 2\text{trace}(\mathbf{S})),$$`
--
 

which suggests the estimator 
`\begin{aligned}
 \hat{\sigma}^2 &amp; = \frac{1}{n + \text{trace}(\mathbf{S}^T\mathbf{S}) - 2\text{trace}(\mathbf{S})} ||\mathbf{Y} - \hat{\mathbf{s}}||^2 \\
 &amp; = \frac{1}{n + \text{trace}(\mathbf{S}^T\mathbf{S}) - 2\text{trace}(\mathbf{S})} \mathrm{RSS}(\hat{\mathbf{s}}).
\end{aligned}`

---
## Arguments

The arguments above rely on the decomposition

`\begin{aligned}
||\widetilde{\mathbf{Y}}-\hat{\mathbf{s}}||^2 &amp; = ||(\widetilde{\mathbf{Y}}-\mathbf{s}) + (\mathbf{s} - \mathbf{S} \mathbf{s})  + (\mathbf{S} \mathbf{s} - \hat{\mathbf{s}})||^2 \\
&amp; =   ||\widetilde{\mathbf{Y}}-\mathbf{s}||^2 + ||\mathbf{s} - \mathbf{S} \mathbf{s}||^2  + ||\mathbf{S} \mathbf{s} - \hat{\mathbf{s}}||^2 \\
&amp;  + \ 2 (\widetilde{\mathbf{Y}}-\mathbf{s})^T  (\mathbf{s} -
\mathbf{S} \mathbf{s}) \\ 
&amp; + \ 2(\widetilde{\mathbf{Y}}-\mathbf{s})^T (\mathbf{S} \mathbf{s} -
\hat{\mathbf{s}}) \\ 
&amp; + \ 2(\mathbf{s} -
\mathbf{S} \mathbf{s})^T (\mathbf{S} \mathbf{s} -
\hat{\mathbf{s}}).
\end{aligned}`

with `\(E\widetilde{\mathbf{Y}} = \mathbf{s}\)`.

First and third cross product have mean zero when `\(E \widetilde{\mathbf{Y}}  = \mathbf{s}\)`. 
If `\(\widetilde{\mathbf{Y}} = \mathbf{Y}^{\text{new}}\)` the second cross product has mean 0 since 
`\(\mathbf{Y}^{\text{new}} \perp \! \! \perp \mathbf{Y}\)`.
If `\(\widetilde{\mathbf{Y}} = \mathbf{Y}\)` the second cross product has mean 
`\(-2\sigma^2 \mathrm{trace}(\mathbf{S})\)`.

---
## Smoothing splines 

The minimizer of 
`$$\text{RSS}(s) = \sum_{i=1}^n (y_i - s(x_i))^2 + \lambda \int s''(t)^2 \mathrm{d} t$$`
is a cubic spline
--

with *knots* in the data points `\(x_i\)`, that is, a function 
`$$f = \sum_i \beta_i \phi_i$$`
where `\(\phi_i\)` are basis functions for the `\(n\)`-dimensional space of such splines. 
--


Cubic splines are piecewise degree 3 polynomials in between knots.

---
## Smoothing splines 

In vector notation 
`$$\hat{\mathbf{s}} = \boldsymbol{\Phi}\hat{\beta}$$`
with `\(\boldsymbol{\Phi}_{ij} = \phi_j(x_i)\)`, 
--

and
`\begin{aligned}
 \text{RSS}(\mathbf{s}, \lambda) &amp; = (\mathbf{Y} - \mathbf{s})^T (\mathbf{Y} - \mathbf{s}) + \lambda \int_a^b s''(t)^2 \mathrm{d} t \\
&amp; = ( \mathbf{Y} -  \boldsymbol{\Phi}\beta)^T (\mathbf{Y} -  \boldsymbol{\Phi}\beta) + \lambda \beta^T \mathbf{\Omega} \beta
\end{aligned}`
--


with 
`$$\mathbf{\Omega}_{ij} = \int \phi_i''(t) \phi_j''(t) \mathrm{d}t.$$`

---
## Smoothing splines 

The minimizer is 
`$$\hat{\beta} = (\boldsymbol{\Phi}^T \boldsymbol{\Phi} + \lambda \mathbf{\Omega}_N)^{-1}\boldsymbol{\Phi}^T \mathbf{Y}$$`

with resulting smoother
`$$\hat{\mathbf{s}} = \boldsymbol{\Phi} ((\boldsymbol{\Phi}^T \boldsymbol{\Phi} + \lambda \mathbf{\Omega})^{-1}\boldsymbol{\Phi}^T \mathbf{Y}.$$`

---
## Smoothing splines in R 

.small[
Smoothing splines are linear smoothers. The basic function in R for computing 
a smoothing spline is `smooth.spline`. It automatically uses LOOCV (or GCV if requested) 
for optimizing the parameter `\(\lambda\)`, but one can also manually specify the 
degree of smoothing using the arguments `spar` or `df` (the latter being the trace of the smoother matrix).

Note that `smooth.spline` "cheats" by default and uses only a subset of the `\(x_i\)`s
as knots for `\(n &gt; 50\)` unless `all.knots = TRUE`. 

The result is an object of class smooth.spline. The `lev` entry contains the diagonals
of the smoother matrix. 

The `gam` function from the mgcv package produces a penalized spline fit,
where the smoothing parameter is selected by GCV by default. The function does not 
attempt to implement smoothing splines exactly, but it provides a vastly more general framework 
for smoothing via generalized additive models and spline basis expansions. 
]

---
## Smoothing spline help

.small[
For the smoothing spline assignment one hurdle is the computation of the matrix `\(\Omega\)`. 
The slides do not explain exactly how to compute this matrix in practice using the `splineDesign`
function. The problem is the boundaries. It is true that there is an `\(n\)`-dimensional basis 
that spans the space of splines solving the optimization problem. It may, however, be more convenient 
to work with a slightly larger space. If the knot sequence consists of the sequence of data points 
with the smallest and largest point each replicated 3 times then the resulting basis is `\((n+2)\)`-dimensional, 
and you can use this basis.

The `splineDesign` function can be used to evaluate the second derivatives of the basis 
functions (by specifying the knots as described above) in an equidistant grid,
which can then be used to compute a Riemann-sum approximation of the integrals. 
This was the procedure I had in mind for the assignment. You can actually quite easily compute 
the integral exactly using Simpson's rule. This is because the integrands are piecewise quadratic. 
However, if you don't want to go into any of this, you are also welcome to install the fda package, 
and then you can compute `\(\Omega\)` by 

`bsplinepen(create.bspline.basis(range(x), breaks = x))`

The important point in this assignment is not how `\(\Omega\)` is computed, but the subsequent computations. 
]

---
## Nadaraya-Watson 

The `ksmooth` function implements the Nadaraya-Watson smoother. There is no 
automatic choice of bandwidth. 

Kernel smoothing for bivariate smoothing is easy to implement along the same 
lines as kernel smoothing for density estimation. Binning can be useful for 
handling large data sets. 

Local regression smoothing can be implemented using `lm` and its `weights` 
argument, or perhaps more efficiently using `lm.wfit`. Refits are in principle 
needed in all points. 

The `locpoly` function in the KernSmooth package does local polynomial 
regression using binning. It does not do automatic bandwidth selection.

For kernel smoothing it is quite easy to compute the diagonals in the smoothing 
matrix and thus LOOCV or GCV.

---
## Loess 

The nonlinear loess estimator is implemented in the `loess` function. It could
be implemented using `lm.wfit`. 

The `loess` function does not automatically select the span but has a default 
of 0.75, which is often too large. 

Since loess is nonlinear, the formulas for linear smoothers do not apply. One can 
instead implement 5- or 10-fold cross validation. 

The `loess` function does return an object with a `trace.hat` entry, which 
might be used as a surrogate for `\(\mathrm{trace}(\mathbf{S})\)` for GCV, say.  

---
## Another loess smooth 



```r
p + stat_smooth(method = "loess", span = 0.5)
```

&lt;img src="BivariateSmoothers_files/figure-html/unnamed-chunk-20-1.png" width="600" height="400" style="display: block; margin: auto;" /&gt;

---
## Running means again


The earlier `runMean` functions is implemented in a low-level way. 
The `filter` function already does the job.


```r
y &lt;- rnorm(7)
rbind(runMean(y, k = 3),
c(filter(y, rep(1/3, 3))))
```

```
##      [,1]      [,2]      [,3]       [,4]      [,5]      [,6] [,7]
## [1,]   NA -1.459354 -1.095819 -0.3111745 0.7881769 0.8963952   NA
## [2,]   NA -1.459354 -1.095819 -0.3111745 0.7881769 0.8963952   NA
```

```r
rbind(runMean(y, k = 5),
c(filter(y, rep(1/5, 5))))
```

```
##      [,1] [,2]       [,3]       [,4]      [,5] [,6] [,7]
## [1,]   NA   NA -0.7421998 -0.1396125 0.1727469   NA   NA
## [2,]   NA   NA -0.7421998 -0.1396125 0.1727469   NA   NA
```

---
## Running means again

A benchmark comparison between `runMean` and `filter` gives 
the following table. 


```
##                               expr      mean    median
## 1        runMean(y[1:512], k = 10)  144.2690  137.2460
## 2       runMean(y[1:1024], k = 10)  267.7698  251.8085
## 3       runMean(y[1:2048], k = 10)  532.1336  513.0300
## 4       runMean(y[1:4196], k = 10) 1081.2695 1043.6090
## 5  filter(y[1:512], rep(1/10, 10))  112.8406  107.1035
## 6 filter(y[1:1024], rep(1/10, 10))  132.7547  126.4620
## 7 filter(y[1:2048], rep(1/10, 10))  227.5572  196.4720
## 8 filter(y[1:4196], rep(1/10, 10))  410.3882  334.0590
```

Thus `filter` is substantially faster than `runMean`.
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightLines": true,
"countIncrementalSlides": false,
"navigation": {
"scroll": false
}
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {window.dispatchEvent(new Event('resize'));});
(function() {var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler"); if (!r) return; s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }"; d.head.appendChild(s);})();</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
  }
});
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
